defaults:
  - _self_
  - override hydra/launcher: submitit_slurm

model_params:
  sh_degree: 3
  source_path:
  model_path:
  images: "images"
  resolution: -1
  white_background: false
  data_device: "cuda"
  eval: false

pipeline_params:
  convert_SHs_python: false
  compute_cov3D_python: false
  debug: false

optimization_params:
  iterations: 20000
  position_lr_init: 0.00016
  position_lr_final: 0.0000016
  position_lr_delay_mult: 0.01
  position_lr_max_steps: 30000
  feature_lr: 0.0025
  opacity_lr: 0.05
  scaling_lr: 0.005
  rotation_lr: 0.001
  percent_dense: 0.01
  lambda_dssim: 0.2
  densification_interval: 25
  opacity_reset_interval: 3000
  densify_from_iter: 25
  densify_until_iter: 18000
  densify_grad_threshold: 0.0002
  random_background: false

eval_params:
  skip_training: false
  skip_rendering: false
  skip_metrics: false
  skip_eval: false
  data_path: "data/MAD-Sim_3dgs"
  epochs: 200
  eval_frequency: 10
  
  
script_params:
  test_iterations: [1500, 7000]
  save_iterations: [1500, 7000]
  checkpoint_iterations: []
  start_checkpoint: null
  meta_model: "meta_model.torch"
  run_name:
  ip: "127.0.0.1"
  port: 6009
  debug_from: -1
  detect_anomaly: false
  quiet: false
  eval_output_path: ""
  
wandb_params:
  project: master
  entity:
  name: null
  group: null
  job_type: null
  id: null
  dir: null
  reinit: false
  tags: null
  notes: null
  anonymous: "never"
  mode: "offline"
  resume: "must"
  force: false
  sync_tensorboard: false
  save_code: true

rl_params:
  rl_lr: 0.01
  meta_model: "meta_model.torch"
  optimizer: "rl_optimizer.torch"
  lr_scheduler: "lr_scheduler.torch"
  train_rl: true
  num_candidates: 2
  hidden_size: 16
  break_reward: -10.0
  reward_function: ["reward_psnr_normalized", "reward_function_2", "reward_function_3", "reward_function_4", "reward_function_5", "reward_psnr_normalized_2", "reward_psnr_normalized_3", "reward_default"]
 #TODO Make all rewards are gotten from rewards.py and then just the one used here used as real reward
 # Otherwhise slurm hydra sweep not working
hydra:
  run:
    dir: ./hydra/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./hydra/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
  launcher:
    timeout_min: 720
    gres: gpu:1
    cpus_per_task: 8
    tasks_per_node: 1
    mem_gb: 48
    nodes: 1
    setup:
        - module load Miniconda3
        - module load GCC/11.2.0
        - module load CUDA/12.3.0
        - export PYTHONPATH=$PYTHONPATH:/bigwork/nhmlhuer/git/gaussian_splatting_rl/src/
        - source activate rl_cloned
    additional_parameters:
    # SLURM specific options
      mail-user: rene.huertgen@stud.uni-hannover.de
      mail-type: BEGIN,END,FAIL

training_script: "train_reinforce.py"
